---
title: T√©cnicas de transforma√ß√£o de recursos
description: Saiba mais sobre t√©cnicas essenciais de pr√©-processamento, como transforma√ß√£o de dados, codifica√ß√£o e dimensionamento de recursos, que preparam dados para o treinamento de modelos estat√≠sticos. Ele aborda a import√¢ncia de lidar com valores ausentes e converter dados categ√≥ricos para melhorar o desempenho e a precis√£o do modelo.
role: Developer
source-git-commit: b248e8f8420b617a117d36aabad615e5bbf66b58
workflow-type: tm+mt
source-wordcount: '3437'
ht-degree: 8%

---

# T√©cnicas de transforma√ß√£o de recursos

As transforma√ß√µes s√£o etapas cruciais de pr√©-processamento que convertem ou dimensionam dados em um formato adequado para treinamento e an√°lise de modelo, garantindo desempenho e precis√£o ideais. Este documento serve como um recurso de sintaxe complementar, fornecendo detalhes detalhados sobre as principais t√©cnicas de transforma√ß√£o de recursos para pr√©-processamento de dados.

Os modelos de aprendizado de m√°quina n√£o podem processar diretamente valores de sequ√™ncia de caracteres ou valores nulos, tornando o pr√©-processamento de dados essencial. Este guia explica como usar v√°rias transforma√ß√µes para imputar valores ausentes, converter dados categ√≥ricos em formatos num√©ricos e aplicar t√©cnicas de dimensionamento de recursos, como codifica√ß√£o e vetoriza√ß√£o one-hot. Esses m√©todos permitem que os modelos interpretem e aprendam efetivamente com os dados, melhorando, em √∫ltima an√°lise, o desempenho.

## Transforma√ß√£o autom√°tica de recursos {#automatic-transformations}

Se voc√™ optar por ignorar a cl√°usula `TRANSFORM` no comando `CREATE MODEL`, a transforma√ß√£o de recursos ocorrer√° automaticamente. O pr√©-processamento autom√°tico de dados inclui transforma√ß√µes de substitui√ß√£o nula e de recursos padr√£o (com base no tipo de dados ). As colunas num√©ricas e de texto s√£o automaticamente imputadas e as transforma√ß√µes de recursos s√£o realizadas para garantir que os dados estejam em um formato adequado para o treinamento do modelo de aprendizado de m√°quina. Esse processo inclui imputa√ß√£o de dados ausentes e transforma√ß√µes categ√≥ricas, num√©ricas e booleanas.

>[!IMPORTANT]
>
>A transforma√ß√£o de recursos utilizada no momento do treino ser√° tamb√©m utilizada para a transforma√ß√£o de recursos no momento da previs√£o e avalia√ß√£o.

As tabelas a seguir explicam como tipos de dados diferentes s√£o tratados quando a cl√°usula `TRANSFORM` √© omitida durante o comando `CREATE MODEL`.

### Substitui√ß√£o nula {#automatic-null-replacement}

| Tipo de dados | Substitui√ß√£o nula |
|-----------------|-----------------------------------------------------|
| Num√©rico | Nulos s√£o substitu√≠dos pelo valor m√©dio da coluna. |
| Categ√≥rico | Nulos s√£o substitu√≠dos pela palavra-chave `ml_unknown`. |
| Booleano | Nulos s√£o substitu√≠dos por um valor `FALSE`. |
| Carimbo de data e hora | Espera-se que esse campo seja cont√≠nuo. |
| Aninhado/STRUCT | A substitui√ß√£o depende do tipo de dados do n√≥ folha. |

### Transforma√ß√£o de recursos {#automatic-feature-transformation}

| Tipo de dados | Transforma√ß√£o de recurso |
|-----------------|-----------------------------------------------------|
| Num√©rico | N√ÉO OBRIGAT√ìRIO - Como esse tipo de dados √© entendido pelos algoritmos de aprendizado de m√°quina do. |
| String | A indexa√ß√£o da string ocorre. |
| Booleano | A indexa√ß√£o da string ocorre. |
| Carimbo de data e hora | Nenhuma opera√ß√£o ocorre. |
| STRUCT | O valor √© expandido para seu n√≥ folha. A transforma√ß√£o ocorre com base no tipo de dados do n√≥ folha. |

**exemplo**

```sql
CREATE model modelname options(model_type='logistic_reg', label='rating') AS SELECT * FROM movie_rating;
```

## Transforma√ß√µes manuais de recursos {#manual-transformations}

Para definir o pr√©-processamento de dados personalizado na instru√ß√£o `CREATE MODEL`, use a cl√°usula `TRANSFORM` em combina√ß√£o com qualquer n√∫mero de fun√ß√µes de transforma√ß√£o dispon√≠veis. Essas fun√ß√µes de pr√©-processamento manual tamb√©m podem ser usadas fora da cl√°usula `TRANSFORM`. Todas as transforma√ß√µes discutidas na [se√ß√£o de transformador abaixo](#available-transformations) podem ser usadas para pr√©-processar os dados manualmente.

### Caracter√≠sticas principais

Estas s√£o as principais caracter√≠sticas da transforma√ß√£o de recursos a serem consideradas ao definir suas fun√ß√µes de pr√©-processamento:

- **Sintaxe**: `TRANSFORM(functionName(colName, parameters) <aliasNAME>)`
   - O nome do alias √© obrigat√≥rio na sintaxe. Voc√™ deve fornecer um nome de alias, caso contr√°rio a consulta falhar√°.

- **Par√¢metros**: os par√¢metros s√£o argumentos posicionais. Isso significa que cada par√¢metro s√≥ pode ter determinados valores. Consulte a documenta√ß√£o relevante para obter detalhes sobre qual fun√ß√£o aceita qual argumento.

- **Transformadores de encadeamento**: a sa√≠da de um transformador pode se tornar a entrada para outro transformador.

- **Uso de recursos**: a √∫ltima transforma√ß√£o de recursos √© usada como um recurso do modelo de aprendizado de m√°quina.

**Exemplo**

```sql
CREATE MODEL modelname 
TRANSFORM(
  string_imputer(language, 'adding_null') AS imp_language, 
  numeric_imputer(users_count, 'mode') AS imp_users_count, 
  string_indexer(imp_language) AS si_lang,  
  vector_assembler(array(imp_users_count, si_lang, watch_minutes)) AS features
)  
OPTIONS(MODEL_TYPE='logistic_reg', LABEL='rating') 
AS SELECT * FROM df;
```

## Transforma√ß√µes dispon√≠veis {#available-transformations}

H√° 19 transforma√ß√µes dispon√≠veis. Essas transforma√ß√µes est√£o divididas em [Transforma√ß√µes gerais](#general-transformations), [Transforma√ß√µes num√©ricas](#numeric-transformations), [Transforma√ß√µes categ√≥ricas](#categorical-transformations) e [Transforma√ß√µes textuais](#textual-transformations).

### Transforma√ß√µes gerais {#general-transformations}

Leia esta se√ß√£o para obter detalhes sobre os transformadores usados para uma grande variedade de tipos de dados. Essas informa√ß√µes s√£o essenciais se voc√™ precisar aplicar transforma√ß√µes que n√£o s√£o espec√≠ficas para dados categ√≥ricos ou textuais.

>[!NOTE]
>
>O tipo de dados de entrada refere-se √† coluna na qual a imputa√ß√£o √© aplicada. O tipo de dados de sa√≠da se refere √† coluna que √© produzida como uma sa√≠da depois que a transforma√ß√£o entra em vigor.

#### Imputador num√©rico {#numeric-imputer}

O transformador **imputador num√©rico** conclui valores ausentes em um conjunto de dados. Usa a m√©dia, a mediana ou o modo das colunas em que os valores ausentes est√£o localizados. As colunas de entrada devem ser `DoubleType` ou `FloatType`. Mais informa√ß√µes e exemplos podem ser encontrados na [documenta√ß√£o do algoritmo Spark](https://spark.apache.org/docs/2.2.0/ml-features.html#imputer).

>[!NOTE]
>
>Todos os valores nulos nas colunas de entrada s√£o tratados como ausentes e, portanto, tamb√©m imputados.

**Tipos de dados**

- Tipo de dados de entrada: Num√©rico
- Tipo de dados de sa√≠da: Num√©rico

**Defini√ß√£o**

```sql
transformer(numeric_imputer(hour, 'mean') hour_imputed)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| `STRATEGY` | Uma estrat√©gia de imputa√ß√£o. Valores dispon√≠veis: [`mean`, `median`, `mode`]. | sequ√™ncia de caracteres | m√©dia | opcional |

{style="table-layout:auto"}

**Exemplo antes da imputa√ß√£o**

| ID | hora |
|---|---|
| 0 | 18,0 |
| 1 | nulo |
| 2 | 8,0 |

**Exemplo ap√≥s imputa√ß√£o (usando estrat√©gia m√©dia)**

| ID | hora |
|---|---|
| 0 | 18,0 |
| 1 | 13,0 |
| 2 | 8,0 |

#### Imputador de string {#string-imputer}

O transformador **imputador de cadeia de caracteres** conclui valores ausentes em um conjunto de dados usando uma cadeia de caracteres fornecida pelo usu√°rio como um argumento de fun√ß√£o. As colunas de entrada e sa√≠da devem ser do tipo de dados `string`.

>[!NOTE]
>
>Todos os valores nulos nas colunas de entrada s√£o tratados como ausentes e substitu√≠dos pela cadeia de caracteres especificada.

**Tipos de dados**

- Tipo de dados de entrada: cadeia de caracteres
- Tipo de dados de sa√≠da: cadeia de caracteres

**Defini√ß√£o**

```sql
transform(string_imputer(name, 'unknown_name') as name_imputed)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| `NULL_REPLACEMENT` | O valor que substitui nulos. | sequ√™ncia de caracteres | ml_unknown | opcional |

{style="table-layout:auto"}

**Exemplo antes da imputa√ß√£o**

| ID | name |
|---|---|
| 0 | John |
| 1 | nulo |
| 2 | Alice |

**Exemplo ap√≥s imputa√ß√£o (usando &#39;ml_unknown&#39; como substitui√ß√£o)**

| ID | name |
|---|---|
| 0 | John |
| 1 | ml_unknown |
| 2 | Alice |

#### Imputador booleano {#imputer}

O transformador **imputador booleano** conclui valores ausentes em um conjunto de dados para uma coluna booleana. As colunas de entrada e sa√≠da devem ser do tipo `Boolean`.

>[!NOTE]
>
>Todos os valores nulos nas colunas de entrada s√£o tratados como ausentes e substitu√≠dos pelo valor booleano especificado.

**Tipos de dados**

- Tipo de dados de entrada: Booleano
- Tipo de dados de sa√≠da: Booleano

**Defini√ß√£o**

```sql
transform(boolean_imputer(name, true) as name_imputed)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| `NULL_REPLACEMENT` | Imputador booleano. Valores permitidos: [`true`, `false`]. | booleano | false | opcional |

**Exemplo antes da imputa√ß√£o**

| ID | sinalizador |
|---|---|
| 0 | true |
| 1 | nulo |
| 2 | false |

**Exemplo ap√≥s imputa√ß√£o (usando &#39;true&#39; como substitui√ß√£o)**

| ID | sinalizador |
|---|---|
| 0 | true |
| 1 | true |
| 2 | false |

#### Montador de vetor {#vector-assembler}

O transformador `VectorAssembler` combina uma lista especificada de colunas de entrada em uma √∫nica coluna de vetor, facilitando o gerenciamento de v√°rios recursos em modelos de aprendizado de m√°quina. Isso √© particularmente √∫til para mesclar recursos brutos e gerados por diferentes transformadores de recursos em um vetor de recursos unificado. `VectorAssembler` aceita colunas de entrada de tipos num√©ricos, booleanos e vetoriais. Em cada linha, os valores das colunas de entrada s√£o concatenados em um vetor na ordem especificada.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#vectorassembler) -->

**Tipos de dados**

- Tipo de dados de entrada: `array[string]` (nomes de coluna com valores num√©ricos/de matriz[num√©ricos])
- Tipo de dados de sa√≠da: `Vector[double]`

**Defini√ß√£o**

```sql
transform(vector_assembler(id, hour, mobile, userFeatures) as features)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
| -------- | ------------ | ----- | -------- | -------- |
| ND | N√£o s√£o necess√°rios par√¢metros adicionais para esse transformador. | ND | ND | ND |

{style="table-layout:auto"}

**Exemplo antes da transforma√ß√£o**

| ID | hora | dispositivos m√≥veis | userFeatures | clicado |
|---|-------|--------|------------------|---------|
| 0 | 18 | 1,0 | [0.0, 10.0, 0.5] | 1,0 |

{style="table-layout:auto"}

**Exemplo ap√≥s a transforma√ß√£o**

| ID | hora | dispositivos m√≥veis | userFeatures | clicado | recursos |
|---|------|--------|------------------|---------|-------------------------------|
| 0 | 18 | 1,0 | [0.0, 10.0, 0.5] | 1,0 | [18.0, 1.0, 0.0, 10.0, 0.5] |

{style="table-layout:auto"}

### Transforma√ß√µes num√©ricas {#numeric-transformations}

Leia esta se√ß√£o para saber mais sobre os transformadores dispon√≠veis para processamento e dimensionamento de dados num√©ricos. Esses transformadores s√£o necess√°rios para manipular e otimizar recursos num√©ricos em seus conjuntos de dados.

#### Binarizador {#binarizer}

O transformador `Binarizer` converte recursos num√©ricos em recursos bin√°rios (0/1) por meio de um processo chamado binariza√ß√£o. Valores de recurso maiores que o limite especificado s√£o convertidos para 1.0, enquanto valores iguais ou inferiores ao limite s√£o convertidos para 0.0. O `Binarizer` d√° suporte aos tipos `Vector` e `Double` para a coluna de entrada.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer). -->

**Tipos de dados**

- Tipo de dados de entrada: coluna num√©rica
- Tipo de dados de sa√≠da: Num√©rico

**Defini√ß√£o**

```sql
transform(numeric_imputer(rating, 'mode') rating_imp, binarizer(rating_imp) rating_binarizer)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|------------|----------------------------------------------------------------------------------------------------------|----------|----------|----------|
| `THRESHOLD` | Par√¢metro do limite usado para binarizar recursos cont√≠nuos. Os recursos maiores que o limite s√£o binarizados para 1.0, enquanto os recursos iguais ou menores que o limite s√£o binarizados para 0.0. | int/double | 0,0 | opcional |

{style="table-layout:auto"}

**Exemplo de entrada antes da binariza√ß√£o**

| ID | avalia√ß√£o |
|---|---------|
| 0 | -18,0 |
| 1 | 13,0 |
| 2 | 8,0 |

**Exemplo de sa√≠da ap√≥s a binariza√ß√£o (limite padr√£o de 0,0)**

| ID | avalia√ß√£o |
|---|---------|
| 0 | 0,0 |
| 1 | 1,0 |
| 2 | 1,0 |

**Defini√ß√£o com limite personalizado**

```sql
transform(numeric_imputer(age, 'mode') age_imp, binarizer(age_imp, 14.0) age_binarizer)
```

**Exemplo de sa√≠da ap√≥s binariza√ß√£o (com um limite de 14.0)**

| ID | idade |
|---|-------|
| 0 | 0,0 |
| 1 | 0,0 |
| 2 | 1,0 |

#### Bucketizer {#bucketizer}

O transformador `Bucketizer` converte uma coluna de recursos cont√≠nuos em uma coluna de compartimentos de recursos, com base em limites especificados pelo usu√°rio. Esse processo √© √∫til para segmentar dados cont√≠nuos em compartimentos ou compartimentos discretos. `Bucketizer` requer um par√¢metro `splits`, que define os limites dos buckets.

**Tipos de dados**

- Tipo de dados de entrada: coluna num√©rica
- Tipo de dados de sa√≠da: Num√©rico (valores agrupados)

**Defini√ß√£o**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|----------|----------|
| `splits` | Um par√¢metro para mapear recursos cont√≠nuos em compartimentos. Com `n+1` divis√µes, h√° `n` compartimentos. As divis√µes devem estar em ordem estritamente crescente, e o intervalo (x,y) √© usado para cada bucket, exceto o √∫ltimo, que inclui y. | matriz (duplo) | N/D | opcional |

{style="table-layout:auto"}

**Exemplos de divis√µes**

- Matriz(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)
- Matriz(0.0, 1.0, 2.0)

As divis√µes devem abranger todo o intervalo de valores Double; caso contr√°rio, os valores fora das divis√µes especificadas ser√£o tratados como erros.

**Exemplo de transforma√ß√£o**

Este exemplo pega uma coluna de recursos cont√≠nuos (`course_duration`), agrupa-a de acordo com o `splits` fornecido e monta os compartimentos resultantes com outros recursos.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

#### MinMaxScaler {#minmaxscaler}

O transformador `MinMaxScaler` redimensiona cada recurso em um conjunto de dados de linhas de vetor para um intervalo especificado, normalmente [0, 1]. Isso garante que todos os recursos contribuam igualmente para o modelo. √â particularmente √∫til para modelos sens√≠veis ao dimensionamento de recursos, como algoritmos baseados em descida de gradiente. O `MinMaxScaler` opera nos seguintes par√¢metros:

- **min**: o limite inferior da transforma√ß√£o, compartilhado por todos os recursos. O padr√£o √© `0.0`.
- **max**: O limite superior da transforma√ß√£o, compartilhado por todos os recursos. O padr√£o √© `1.0`.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler).  -->

**Tipos de dados**

- Tipo de dados de entrada: `Array[Double]`
- Tipo de dados de sa√≠da: `Array[Double]`

**Defini√ß√£o**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------|--------------------------------------------------------------------------------------------------|------|---------|----------|
| `min` | Limite inferior ap√≥s a transforma√ß√£o, compartilhado por todos os recursos. | duplo | 0,0 | opcional |
| `max` | Limite superior ap√≥s a transforma√ß√£o, compartilhado por todos os recursos. | duplo | 1,0 | opcional |

**Exemplo de transforma√ß√£o**

Este exemplo transforma um conjunto de recursos, redimensionando-os para o intervalo especificado usando MinMaxScaler ap√≥s aplicar v√°rias outras transforma√ß√µes.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

#### MaxAbsScaler {#maxabsscaler}

O transformador `MaxAbsScaler` redimensiona cada recurso em um conjunto de dados de linhas de vetor para o intervalo [-1, 1] dividindo pelo valor absoluto m√°ximo de cada recurso. Essa transforma√ß√£o √© ideal para preservar a dispers√£o em conjuntos de dados com valores positivos e negativos, pois n√£o altera ou centraliza os dados. Isso torna o `MaxAbsScaler` particularmente adequado para modelos que s√£o sens√≠veis √† escala de recursos de entrada, como aqueles que envolvem c√°lculos de dist√¢ncia.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#maxabsscaler). -->

**Tipos de dados**

- Tipo de dados de entrada: `Array[Double]`
- Tipo de dados de sa√≠da: `Array[Double]`

**Defini√ß√£o**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------|---------------------------------------------------------------------------------------------------------|------|---------|----------|
| ND | MaxAbsScaler n√£o requer par√¢metros adicionais para sua opera√ß√£o. | ND | ND | ND |

**Exemplo de transforma√ß√£o**

Este exemplo aplica v√°rias transforma√ß√µes, incluindo `MaxAbsScaler`, para redimensionar recursos no intervalo [-1, 1].

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling)
```

#### Normalizador {#normalizer}

`Normalizer` √© um transformador que normaliza cada vetor em um conjunto de dados de linhas de vetor para ter uma norma de unidade. Esse processo garante uma escala consistente sem alterar a dire√ß√£o dos vetores. Essa transforma√ß√£o √© particularmente √∫til em modelos de aprendizado de m√°quina que dependem de medidas de dist√¢ncia ou outros c√°lculos baseados em vetores, especialmente quando a magnitude dos vetores varia significativamente.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#normalizer) -->

**Tipos de dados**

- Tipo de dados de entrada: `array[double]` / `vector[double]`
- Tipo de dados de sa√≠da: `vector[double]`

**Defini√ß√£o**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, normalizer(vec_assembler, 3) as normalized)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------|----------------------------------------------------------------------------------------|---------|---------|----------|
| `p` | Especifica o `p-norm` usado para normaliza√ß√£o (por exemplo, `1-norm`, `2-norm`, etc.). | inteiro | 2 | opcional |

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como aplicar v√°rias transforma√ß√µes, incluindo o `Normalizer`, para normalizar um conjunto de recursos usando o `p-norm` especificado.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, normalizer(vec_assembler, 3) as normalized)
```

#### QuantileDiscretizer {#quantilediscretizer}

O `QuantileDiscretizer` √© um transformador que converte uma coluna com recursos cont√≠nuos em recursos categ√≥ricos agrupados, com o n√∫mero de agrupamentos determinado pelo par√¢metro `numBuckets`. Em alguns casos, o n√∫mero real de buckets pode ser menor do que o n√∫mero especificado se houver poucos valores distintos para criar quantidades suficientes.

Essa transforma√ß√£o √© particularmente √∫til para simplificar a representa√ß√£o de dados cont√≠nuos ou prepar√°-la para algoritmos que funcionam melhor com entradas categ√≥ricas.

**Tipos de dados**

- Tipo de dados de entrada: coluna num√©rica
- Tipo de dados de sa√≠da: coluna num√©rica (categ√≥rica)

**Defini√ß√£o**

```sql
TRANSFORM(quantile_discretizer(hour, 3) as result)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|--------------|--------------------------------------------------------------------------------------------------------------------------|---------|---------|----------|
| `NUM_BUCKETS` | O n√∫mero de buckets (quantis ou categorias) nos quais os pontos de dados s√£o agrupados. Esse n√∫mero deve ser maior ou igual a dois. | inteiro | 2 | opcional |

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como o `QuantileDiscretizer` agrupa uma coluna de recursos cont√≠nuos (`hour`) em tr√™s compartimentos categ√≥ricos.

```sql
TRANSFORM(quantile_discretizer(hour, 3) as result)
```

**Exemplo antes e depois da diferencia√ß√£o**

| ID | hora | resultado |
|---|------|--------|
| 0 | 18,0 | 2.0 |
| 1 | 19,0 | 2.0 |
| 2 | 8,0 | 1,0 |
| 3 | 5.0 | 1,0 |
| 4 | 2,2 | 0,0 |

#### StandardScaler {#standardscaler}

O `StandardScaler` √© um transformador que normaliza cada recurso em um conjunto de dados de linhas de vetor para ter um desvio padr√£o de unidade e/ou m√©dia zero. Esse processo torna os dados mais adequados para algoritmos que assumem que os recursos s√£o centralizados em torno de zero com uma escala consistente. Essa transforma√ß√£o √© particularmente importante para modelos de aprendizado de m√°quina como SVM, regress√£o log√≠stica e redes neurais, onde dados n√£o padronizados podem levar a problemas de converg√™ncia ou precis√£o reduzida.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#standardscaler).  -->

**Tipos de dados**

- Tipo de dados de entrada: vetor
- Tipo de dados de sa√≠da: vetor

**Defini√ß√£o**

```sql
TRANSFORM(standard_scaler(feature) as ss_features)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|------------|------------------------------------------------------------------------------------------------------|---------|---------|----------|
| `withStd` | Dimensiona os dados para ter o desvio padr√£o da unidade. | booleano | Verdadeiro | opcional |
| `withMean` | Centraliza os dados com a m√©dia antes do dimensionamento. Essa op√ß√£o produz sa√≠da densa, portanto, tenha cuidado com entradas esparsas. | booleano | Falso | opcional |

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como aplicar o StandardScaler a um conjunto de recursos, normalizando-os com desvio padr√£o de unidade e m√©dia zero.

```sql
TRANSFORM(standard_scaler(feature) as ss_features)
```

### Transforma√ß√µes categ√≥ricas {#categorical-transformations}

Leia esta se√ß√£o para obter uma vis√£o geral dos transformadores dispon√≠veis criados para converter e pr√©-processar dados categ√≥ricos para modelos de aprendizado de m√°quina do. Essas transforma√ß√µes s√£o projetadas para pontos de dados que representam categorias ou r√≥tulos distintos, em vez de valores num√©ricos.

#### IndexadorDeCadeiaDeCaracteres {#stringindexer}

`StringIndexer` √© um transformador que codifica uma coluna de r√≥tulos de cadeia de caracteres em uma coluna de √≠ndices num√©ricos. Os √≠ndices variam de 0 a `numLabels` e s√£o ordenados por frequ√™ncia de r√≥tulo (o r√≥tulo mais frequente recebe um √≠ndice de 0). Se a coluna de entrada for num√©rica, ela ser√° convertida em uma string antes da indexa√ß√£o. R√≥tulos n√£o vistos podem ser atribu√≠dos ao √≠ndice `numLabels` se especificados pelo usu√°rio.

Essa transforma√ß√£o √© particularmente √∫til para converter dados de sequ√™ncia categ√≥rica em forma num√©rica, tornando-a adequada para modelos de aprendizado de m√°quina que exigem entrada num√©rica.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer) -->

**Tipos de dados**

- Tipo de dados de entrada: cadeia de caracteres
- Tipo de dados de sa√≠da: Num√©rico

**Defini√ß√£o**

```sql
TRANSFORM(string_indexer(category) as si_category)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------|-------------|------|---------|----------|
| ND | `StringIndexer` n√£o requer par√¢metros adicionais para sua opera√ß√£o. | ND | ND | ND |

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como aplicar o `StringIndexer` a um recurso categ√≥rico, convertendo-o em um √≠ndice num√©rico.

```sql
TRANSFORM(string_indexer(category) as si_category)
```

#### OneHotEncoder {#onehotencoder}

`OneHotEncoder` √© um transformador que converte uma coluna de √≠ndices de r√≥tulos em uma coluna de vetores bin√°rios esparsos, onde cada vetor tem no m√°ximo um √∫nico valor. Essa codifica√ß√£o √© particularmente √∫til para permitir que algoritmos que exigem entrada num√©rica, como Regress√£o l√≥gica, incorporem dados categ√≥ricos de maneira eficaz.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#onehotencoder).  -->

**Tipos de dados**

- Tipo de dados de entrada: Num√©rico
- Tipo de dados de sa√≠da: Vetor[Int]

**Defini√ß√£o**

```sql
TRANSFORM(string_indexer(category) as si_category, one_hot_encoder(si_category) as ohe_category)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------|-------------|------|---------|----------|
| ND | O OneHotEncoder n√£o requer par√¢metros adicionais para sua opera√ß√£o. | ND | ND | ND |

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como aplicar primeiro o `StringIndexer` a um recurso categ√≥rico e, em seguida, usar o `OneHotEncoder` para converter os valores indexados em um vetor bin√°rio.

```sql
TRANSFORM(string_indexer(category) as si_category, one_hot_encoder(si_category) as ohe_category)
```

### Transforma√ß√µes textuais {#textual-transformations}

Esta se√ß√£o fornece detalhes sobre os transformadores dispon√≠veis para processar e converter dados de texto em formatos que podem ser usados por modelos de aprendizado de m√°quina. Esta se√ß√£o √© crucial para desenvolvedores que trabalham com dados de linguagem natural e an√°lise de texto.

#### CountVetorizer {#countvectorizer}

O `CountVectorizer` √© um transformador que converte uma cole√ß√£o de documentos de texto em vetores de contagem de tokens, produzindo representa√ß√µes esparsas com base no vocabul√°rio extra√≠do do corpus. Essa transforma√ß√£o √© essencial para converter dados de texto em um formato num√©rico que pode ser usado por algoritmos de aprendizado de m√°quina, como LDA (Latent Dirichlet Allocation), representando a frequ√™ncia de tokens em cada documento.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#countvectorizer). -->

**Tipos de dados**

- Tipo de dados de entrada: Matriz[Cadeia de caracteres]
- Tipo de dados de sa√≠da: Vetor denso

**Defini√ß√£o**

```sql
TRANSFORM(count_vectorizer(texts) as cv_output)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|---------|----------|
| `VOCAB_SIZE` | Tamanho m√°ximo do vocabul√°rio. CountVetorizer cria um vocabul√°rio que considera apenas os `vocabSize` termos principais ordenados por frequ√™ncia de termo no corpo. | Int | 218 | opcional |
| `MIN_DOC_FREQ` | Especifica o n√∫mero m√≠nimo de documentos diferentes que um termo deve aparecer para ser inclu√≠do no vocabul√°rio. Pode ser um n√∫mero absoluto ou uma fra√ß√£o de documentos (se for duplo). | Duplo | 1,0 | opcional |
| `MAX_DOC_FREQ` | Especifica o n√∫mero m√°ximo de documentos diferentes que um termo pode aparecer para ser inclu√≠do no vocabul√°rio. Pode ser um n√∫mero absoluto ou uma fra√ß√£o de documentos (se for duplo). | Duplo | (263)-1 | opcional |
| `MIN_TERM_FREQ` | Filtra palavras raras em um documento. Os termos com frequ√™ncia/contagem menor que o limite fornecido s√£o ignorados. Pode ser um n√∫mero absoluto ou uma fra√ß√£o da contagem de tokens do documento. | Duplo | 1,0 | opcional |

{style="table-layout:auto"}

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como o CountVetorizer converte uma cole√ß√£o de matrizes de texto em vetores de contagens de token, produzindo uma representa√ß√£o esparsa.

```sql
TRANSFORM(count_vectorizer(texts) as cv_output)
```

**Exemplo antes e depois da vetoriza√ß√£o**

| ID | textos | cv_output |
|----|---------------------------------|-----------------------------------|
| 0 | Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) | (3,[0,1,2],[1.0,1.0,1.0]) |
| 1 | Array(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;) | (3,[0,1,2],[2.0,2.0,1.0]) |

{style="table-layout:auto"}

#### NGram {#ngram}

O `NGram` √© um transformador que gera uma sequ√™ncia de n-gramas, onde um n-grama √© uma sequ√™ncia de tokens (&#39;??&#39;) (normalmente palavras) para algum inteiro (`ùëõ`). A sa√≠da consiste em strings delimitadas por espa√ßos de &#39;??&#39; palavras consecutivas, que podem ser usadas como recursos em modelos de aprendizado de m√°quina, particularmente aqueles focados no processamento de linguagem natural.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#n-gram). -->

**Tipos de dados**

- Tipo de dados de entrada: Matriz[Cadeia de caracteres]
- Tipo de dados de sa√≠da: Matriz[Cadeia]

**Defini√ß√£o**

```sql
TRANSFORM(tokenizer(review_comments) as token_comments, ngram(token_comments, 3) as n_tokens)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------|-----------------------------------------------------------------------------------------------|---------|-------------------|----------|
| `N` | O comprimento m√≠nimo de n-gramas deve ser maior ou igual a 1. | inteiro | 2 (recursos de bigrama) | opcional |

{style="table-layout:auto"}

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como o transformador NGram cria uma sequ√™ncia de 3 gramas de uma lista de tokens derivados de dados de texto.

```sql
TRANSFORM(tokenizer(review_comments) as token_comments, ngram(token_comments, 3) as n_tokens)
```

**Exemplo antes e depois da transforma√ß√£o n-grama**

| ID | textos | n_tokens |
|----|-------------------------------------------------------|-------------------------------------------------------|
| 0 | [&quot;este&quot;, &quot;era&quot;, &quot;um&quot;, &quot;entretenimento&quot;, &quot;filme&quot;] | [&quot;este foi um&quot;, &quot;foi um filme divertido&quot;, &quot;um filme divertido&quot;] |

{style="table-layout:auto"}

#### StopWordsRemover {#stopwordsremover}

O `StopWordsRemover` √© um transformador que remove palavras de interrup√ß√£o de uma sequ√™ncia de cadeias de caracteres, filtrando palavras comuns que n√£o t√™m significado significativo. Ele assume como entrada uma sequ√™ncia de cadeias de caracteres (como a sa√≠da de um tokenizador) e remove todas as palavras de interrup√ß√£o especificadas pelo par√¢metro `stopWords`.

Essa transforma√ß√£o √© √∫til para pr√©-processar dados de texto, melhorando a efic√°cia de modelos de aprendizado de m√°quina downstream ao eliminar palavras que n√£o contribuem muito para o significado geral.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#stopwordsremover) -->

**Tipos de dados**

- Tipo de dados de entrada: Matriz[Cadeia de caracteres]
- Tipo de dados de sa√≠da: Matriz[Cadeia]

**Defini√ß√£o**

```sql
TRANSFORM(stop_words_remover(raw) as filtered)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|--------------------|--------------------------------------------------------------------------------------------------|---------------|-------------------------|----------|
| `stopWords` | As palavras a serem filtradas. | matriz [cadeia] | Por padr√£o: Ingl√™s stop words | opcional |

{style="table-layout:auto"}

<!-- Q) should this be the `CUSTOM_STOP_WORDS` parameter or the `stopWords` parameter?  -->

**Exemplo de transforma√ß√£o**

Este exemplo mostra como o `StopWordsRemover` filtra palavras de interrup√ß√£o comuns em ingl√™s de uma lista de tokens.

```sql
TRANSFORM(stop_words_remover(raw) as filtered)
```

**Exemplo antes e depois da remo√ß√£o de palavras de interrup√ß√£o**

| ID | raw | filtrado |
|----|------------------------------|--------------------------|
| 0 | [Eu, vi, o, vermelho, bal√£o] | [serra, vermelha, bal√£o] |
| 1 | [Maria, teve, um, pequeno, cordeiro] | [Maria, pequena, cordeiro] |

**Exemplo com palavras de interrup√ß√£o personalizadas**

Este exemplo demonstra como usar uma lista personalizada de palavras de interrup√ß√£o para filtrar palavras espec√≠ficas das sequ√™ncias de entrada.

```sql
TRANSFORM(stop_words_remover(raw, array("red", "I", "had")) as filtered)
```

**Exemplo antes e depois da remo√ß√£o de palavras de interrup√ß√£o personalizadas**

| ID | raw | filtrado |
|----|------------------------------|--------------------------|
| 0 | [Eu, vi, o, vermelho, bal√£o] | [viu, o, bal√£o] |
| 1 | [Maria, teve, um, pequeno, cordeiro] | [Maria, pequena, cordeiro] |

#### TF- IDF {#tf-idf}

O `TF-IDF` (Frequ√™ncia de termo-frequ√™ncia inversa do documento) √© um transformador usado para medir a import√¢ncia de uma palavra dentro de um documento relativo a um corpo. Frequ√™ncia do termo (TF) refere-se ao n√∫mero de vezes que um termo \(t\) aparece em um documento \(d\), enquanto frequ√™ncia do documento (DF) mede quantos documentos no corpo \(D\) cont√™m o termo \(t\). Esse m√©todo √© amplamente utilizado na minera√ß√£o de texto para reduzir a influ√™ncia de palavras comuns, como &quot;a&quot;, &quot;o&quot; e &quot;de&quot;, que trazem pouca informa√ß√£o √∫nica.

Essa transforma√ß√£o √© particularmente valiosa em tarefas de minera√ß√£o de texto e processamento de linguagem natural, pois atribui um valor num√©rico √† import√¢ncia de cada palavra em um documento e em todo o corpo.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf) -->

**Tipos de dados**

- Tipo de dados de entrada: Matriz[Cadeia de caracteres]
- Tipo de dados de sa√≠da: Vetor[Int]

**Defini√ß√£o**

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------------|----------------------------------------------------------------------------------------|------|---------|----------|
| `NUM_FEATURES` | O n√∫mero de recursos a serem gerados. Deve ser superior a 0. | Int | 262144 | opcional |
| `MIN_DOC_FREQ` | O n√∫mero m√≠nimo de documentos nos quais um termo deve parecer estar inclu√≠do no modelo. | Int | 0 | opcional |

{style="table-layout:auto"}

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como usar TF-IDF para transformar frases tokenizadas em um vetor de recursos que representa a import√¢ncia de cada termo no contexto de todo o corpo.

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

#### Tokenizer {#tokenizer}

O `Tokenizer` √© um transformador que divide o texto, como uma frase, em termos individuais, geralmente palavras. Ele converte senten√ßas em matrizes de tokens, fornecendo uma etapa fundamental no pr√©-processamento de texto que prepara os dados para an√°lise de texto adicional ou processos de modelagem.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#tokenizer) -->

**Tipos de dados**

- Tipo de dados de entrada: senten√ßa textual
- Tipo de dados de sa√≠da: Matriz[Cadeia]

**Defini√ß√£o**

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|-----------|-------------|------|---------|----------|
| ND | O `Tokenizer` n√£o requer par√¢metros adicionais para sua opera√ß√£o. | ND | ND | ND |

**Exemplo de transforma√ß√£o**

Este exemplo demonstra como o `Tokenizer` divide frases em palavras individuais (tokens) como parte de um pipeline de processamento de texto.

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

#### Word2Vec {#word2vec}

O `Word2Vec` √© um estimador que processa sequ√™ncias de palavras representando documentos e treina um `Word2VecModel`. Esse modelo mapeia cada palavra para um vetor de tamanho fixo exclusivo e transforma cada documento em um vetor, calculando a m√©dia dos vetores de todas as palavras no documento. Amplamente usado em tarefas de processamento de linguagem natural, o `Word2Vec` cria incorpora√ß√µes de palavras que capturam o significado sem√¢ntico, convertendo dados de texto em vetores num√©ricos que representam as rela√ß√µes entre as palavras e habilitando an√°lises de texto e modelos de aprendizado de m√°quina mais eficazes.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec) -->

**Tipos de dados**

- Tipo de dados de entrada: Matriz[Cadeia de caracteres]
- Tipo de dados de sa√≠da: Vetor[Double]

**Defini√ß√£o**

```sql
TRANSFORM(tokenizer(review) as tokenized, word2Vec(tokenized, 10, 1) as word2Vec)
```

**Par√¢metros**

| Par√¢metro | Descri√ß√£o | Tipo | Padr√£o | Opcional |
|--------------|-----------------------------------------------------------------------------------------------------|---------|---------|----------|
| `VECTOR_SIZE` | A dimens√£o do vetor em que cada palavra √© transformada. | N√∫mero inteiro | 100 | opcional |
| `MIN_COUNT` | O n√∫mero m√≠nimo de vezes que um token deve aparecer para ser inclu√≠do no vocabul√°rio do modelo `Word2Vec`. | N√∫mero inteiro | 5 | opcional |

{style="table-layout:auto"}

**Exemplo de transforma√ß√£o**

Este exemplo mostra como `Word2Vec` converte uma revis√£o tokenizada em um vetor de tamanho fixo que representa a m√©dia dos vetores de palavras no documento.

```sql
TRANSFORM(tokenizer(review) as tokenized, word2Vec(tokenized, 10, 1) as word2Vec)
```

**Exemplo antes e depois da transforma√ß√£o do Word2Vec**

| revis√£o | tokenizado | word2Vec |
|-------------------------------|--------------------------------------|---------------------------------|
| este foi um filme divertido | [este, foi, um, divertido, filme] | [-0.025713888928294182,0.00818799751577899,0.0092235435731709,-0.0151538523377 7133,0.012175946310162545,3.1129065901041035E-4,0.0025145105042611252,0.0057570 19785232843,-0.021328244300093502,0.009335877187550069] |

{style="table-layout:auto"}
